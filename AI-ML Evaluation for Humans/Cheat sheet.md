### 1. Human-Friendly Metrics Cheat Sheet
A single table summarizing some popular AI-ML metrics, their meaning, when to use them, and how to interpret results.

---

### üí° How to Read This Cheat Sheet

This table isn‚Äôt just a list of metrics ‚Äî it‚Äôs a **human lens on ML performance**. Here‚Äôs how to approach it:

1. **Start with the problem type:** Is your model forecasting, predicting a number, or classifying categories? Check the **Type** column first.  
2. **Understand what the metric tells you:** Look at the ‚ÄúWhat it tells you‚Äù and ‚ÄúThink of it as‚Äù columns to translate numbers into actionable meaning.  
3. **Decide when to use it:** Not all metrics are useful for every scenario. Use the **When to use it** guidance to pick the right one for your context.  
4. **Interpret the results:** The **Example** and **Evaluation** columns provide a practical sense of whether your model is performing well. Remember, metrics are tools, not goals.  
5. **Check the sources:** If you want to dive deeper, the **Source** column points you to papers, textbooks, or documentation for reference.

> ‚ö†Ô∏è Tip: Always combine metrics with **context and domain knowledge**. A ‚Äúgood‚Äù metric in isolation might be misleading if it doesn‚Äôt align with your business objective.

By following these steps, you‚Äôll move from raw numbers to **insightful, actionable understanding** of your ML models ‚Äî without drowning in formulas or confusion.

---

### üìö The Cheat Sheet:

| Metric                   | Type                     | What it tells you                                                         | Think of it as                                                    | When to use it                                                                  | Example                                                 | Evaluation                                                        | Source                                              |
| ------------------------ | ------------------------ | ------------------------------------------------------------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------- | ------------------------------------------------------- | ----------------------------------------------------------------- | --------------------------------------------------- |
| MAPE                     | Forecasting              | Average size of prediction errors as a % of actual values                 | "By what % do I usually miss?"                                    | Business forecasts, easy-to-explain metric; not good if actuals ‚âà 0             | 12% ‚Üí forecasts are off 12% on average                  | <10% Excellent; 10‚Äì20% Good; 20‚Äì50% Fair; >50% Poor               | Lewis (1982); Moreno et al. (2013)                  |
| MASE                     | Forecasting              | How model performs vs. naive forecast                                     | "Am I better than guessing yesterday‚Äôs value?"                    | Comparing series, intermittent demand                                           | 0.8 ‚Üí model is 20% better than naive                    | <1 Better; =1 Same; >1 Worse                                      | Hyndman & Koehler (2006)                            |
| sMAPE                    | Forecasting              | Average forecast error relative to forecast & actual                      | "How far off am I, %‚Äëwise, without tiny values distorting?"       | Small/zero values, competitions                                                 | 15% ‚Üí forecasts differ 15% on average                   | Lower = better                                                    | Armstrong & Collopy (1992); Makridakis competitions |
| RMSE                     | Forecasting / Regression | Typical size of prediction errors in data units; big errors weighted more | "How big are my mistakes, with large errors punished more?"       | ML regression, costly errors                                                    | 5 ‚Üí predictions usually 5 units off                     | Compare to data scale; smaller better                             | Hyndman & Athanasopoulos (2018)                     |
| MAE                      | Forecasting / Regression | Average absolute difference between predictions and actuals               | "How far off am I on average?"                                    | Simple, interpretable error in same units; less sensitive to outliers than RMSE | MAE = 4 ‚Üí predictions are 4 units off                   | Lower = better                                                    | Hyndman & Athanasopoulos (2018)                     |
| Median Absolute Error    | Forecasting / Regression | Median of all absolute differences                                        | "Typical error ignoring outliers"                                 | Data with occasional extreme spikes                                             | Median AE = 3 ‚Üí most errors are within 3 units          | Lower = better                                                    | scikit-learn docs                                   |
| R¬≤                       | Regression               | Proportion of variance explained by model                                 | "How much of the story does my model capture?"                    | Regression model fit                                                            | 0.80 ‚Üí explains 80% variance                            | <0.25 Very weak; 0.25‚Äì0.50 Weak; 0.50‚Äì0.75 Moderate; >0.75 Strong | Chin (1998); Hair et al. (2011)                     |
| Adjusted R¬≤              | Regression               | R¬≤ adjusted for number of predictors                                      | "How much variance is explained, accounting for model complexity" | Regression with multiple features                                               | 0.78 ‚Üí slightly lower than R¬≤ to account for predictors | Same rule-of-thumb as R¬≤                                          | Draper & Smith (1998)                               |
| Precision                | Classification           | Fraction of predicted positives that are correct                          | "When I say yes, how often am I right?"                           | False positives costly                                                          | 0.9 ‚Üí 90% of predicted positives correct                | Higher = better                                                   | Manning, Raghavan & Sch√ºtze (2008)                  |
| Recall                   | Classification           | Fraction of actual positives correctly predicted                          | "Of all the real positives, how many did I catch?"                | Missing positives costly                                                        | 0.8 ‚Üí 80% of true positives detected                    | Higher = better                                                   | Manning, Raghavan & Sch√ºtze (2008)                  |
| F1-score                 | Classification           | Harmonic mean of precision & recall                                       | "Overall accuracy for positive predictions"                       | Imbalanced datasets; balance precision & recall                                 | 0.85 ‚Üí good balance                                     | Higher = better (0‚Äì1)                                             | Manning, Raghavan & Sch√ºtze (2008); Powers (2011)   |
| Accuracy                 | Classification           | Fraction of correct predictions                                           | "How often am I right overall?"                                   | Balanced classification datasets                                                | 0.92 ‚Üí 92% correct                                      | Higher = better (0‚Äì1)                                             | scikit-learn docs                                   |
| Specificity              | Classification           | Fraction of actual negatives correctly predicted                          | "Of all the real negatives, how many did I catch?"                | When false positives are costly                                                 | 0.95 ‚Üí 95% of negatives correctly identified            | Higher = better                                                   | scikit-learn docs                                   |
| ROC-AUC                  | Classification           | Ability of model to distinguish classes                                   | "How well can I separate positive & negative cases?"              | Imbalanced datasets; threshold-independent                                      | 0.88 ‚Üí model separates classes well                     | Higher = better (0‚Äì1)                                             | Fawcett (2006)                                      |
| PR-AUC                   | Classification           | Area under precision-recall curve                                         | "Overall trade-off between precision & recall"                    | Imbalanced datasets with rare positives                                         | 0.82 ‚Üí good balance between precision & recall          | Higher = better (0‚Äì1)                                             | Davis & Goadrich (2006)                             |
| Log Loss / Cross-Entropy | Classification           | Penalizes confident wrong predictions                                     | "How surprised is the model when it‚Äôs wrong?"                     | Classification models with probability outputs                                  | 0.35 ‚Üí reasonably low loss                              | Lower = better                                                    | Bishop (2006)                                       |
